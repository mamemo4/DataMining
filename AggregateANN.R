# Get training data
agg_data <- read.csv('agg_data.csv')

# Split data for 10 cross validation
library(caTools)
set.seed(26)
split <- sample.split(agg_data, SplitRatio = 0.8)
training_set <- subset(agg_data, split == TRUE)
test_set <- subset(agg_data, split == FALSE)

# Feature Scaling
#training_set[-5] <- scale(training_set[-5])
#test_set[-5] <- scale(test_set[-5])

# Here is the test_set:

# GDP Births Unemp        SP GiftAmount
# 3   5.263263   15.6   5.6  108.4517   145800.0
# 8   4.608597   15.1   7.1   98.4950   306408.7
# 13 -1.910891   15.9   9.7  118.7325  1074470.0
# 18  3.461748   15.7   6.2  285.4517  3852486.7
# 23  3.555396   15.8   7.5  415.5583 20404699.7
# 28  4.487026   14.2   4.9  856.7217 34701799.3
# 33  1.786128   14.0   5.8 1010.9433 36398157.6
# 38  1.778570   14.3   4.6 1473.5883 29982072.8
# 43  2.224031   12.6   8.1 1372.5866 21199268.6

# Here is the training_set

# GDP Births Unemp         SP GiftAmount
# 1   3.20680726   18.4   5.0   83.44333   170497.4
# 2   3.29547673   17.2   6.0   97.41667    28835.0
# 4   5.64312485   14.8   4.9  108.19000   141100.0
# 5  -0.51715456   14.8   5.6   83.88417   140800.0
# 6  -0.19767854   14.6   8.5   85.34750   340760.0
# 7   5.38609005   14.6   7.7  101.35167   135275.1
# 9   5.56168493   15.0   6.1   95.48667   531970.1
# 10  3.17569075   15.6   5.9  102.32750  1426184.9
# 11 -0.24459623   15.9   7.2  117.26000  1354817.0
# 12  2.59447039   15.8   7.6  128.97583  1858002.1
# 14  4.63245718   15.6   9.6  158.70667  1351427.8
# 15  7.25908696   15.6   7.5  160.12083  2447543.0
# 16  4.23873752   15.8   7.2  185.29167  5258562.4
# 17  3.51161450   15.6   7.0  236.22250  5600125.9
# 19  4.20397198   16.0   5.5  265.49750  5168754.7
# 20  3.68052403   16.4   5.3  320.00833 45420519.0
# 21  1.91937030   16.7   5.6  334.61833 10385117.2
# 22 -0.07408453   16.2   6.9  374.28416 15265818.5
# 24  2.74585672   15.4   6.9  450.90500 23660352.8
# 25  4.03764342   15.0   6.1  461.24250 20524136.6
# 26  2.71897579   14.6   5.6  533.82084 20047500.3
# 27  3.79588123   14.4   5.4  664.44500 35176570.6
# 29  4.44991096   14.3   4.5 1066.28917 35533197.0
# 30  4.68519961   14.2   4.2 1310.58166 36163097.7
# 31  4.09217645   14.4   4.0 1432.14333 40327472.2
# 32  0.97598183   14.1   4.7 1200.10331 40183870.1
# 34  2.80677596   14.1   6.0  948.58833 23370875.9
# 35  3.78574285   14.0   5.5 1125.62833 21104987.2
# 36  3.34521606   14.0   5.1 1204.73917 28901718.4
# 37  2.66662583   14.3   4.6 1304.93085 22919104.0
# 39 -0.29162146   14.0   5.8 1262.03415 22972986.0
# 40 -2.77552957   13.5   9.3  930.48168 18753433.7
# 41  2.53192062   13.0   9.6 1120.58916 19830731.8
# 42  1.60145467   12.7   8.9 1281.39666 11894799.2
# 44  1.67733153   12.4   7.4 1618.35251 19682181.6
# 45  2.56919359   12.5   6.2 1926.66917 20931070.1
# 46  2.86158702   12.4   5.3 2054.11413 22455651.5
# 47  1.48527919   12.4   4.9 2088.91332 27749510.2


# Neural Net
#install.packages('h2o')
library(h2o)
h2o.init(nthreads = -1)

NN <- h2o.deeplearning(y = 'GiftAmount', 
                       training_frame = as.h2o(training_set),
                       activation = 'Rectifier',
                       hidden = c(2),
                       epochs = 100,
                       train_samples_per_iteration = -2)

# Predicting the test set results
new_predict <- h2o.predict(NN, newdata = as.h2o(test_set[-5]))

# Table showing the differences in test data outcomes
predictions <- matrix(c(test_set[,5], new_predict), nrow = 10, ncol = 2)

predictions
# Here is what this gives us for new years:

# [,1]     [,2]    
# [1,] 145800   145800  
# [2,] 306408.7 306408.7
# [3,] 1074470  1074470 
# [4,] 3852487  3852487 
# [5,] 20404700 20404700
# [6,] 34701799 34701799
# [7,] 36398158 36398158
# [8,] 29982073 29982073
# [9,] 21199269 21199269
# [10,] ?        ?       It makes me do +1 row..something about formatting


# Predicting the training set
train_predict <- h2o.predict(NN, newdata = as.h2o(training_set[-5]))

test <- matrix(c(training_set[,5], train_predict), nrow = 39, ncol = 2)

test
# Here is what this gives me for the training data predictions
# [,1]     [,2]    
# [1,] 170497.4 170497.4
# [2,] 28835    28835   
# [3,] 141100   141100  
# [4,] 140800   140800  
# [5,] 340760   340760  
# [6,] 135275.1 135275.1
# [7,] 531970.1 531970.1
# [8,] 1426185  1426185 
# [9,] 1354817  1354817 
# [10,] 1858002  1858002 
# [11,] 1351428  1351428 
# [12,] 2447543  2447543 
# [13,] 5258562  5258562 
# [14,] 5600126  5600126 
# [15,] 5168755  5168755 
# [16,] 45420519 45420519
# [17,] 10385117 10385117
# [18,] 15265819 15265819
# [19,] 23660353 23660353
# [20,] 20524137 20524137
# [21,] 20047500 20047500
# [22,] 35176571 35176571
# [23,] 35533197 35533197
# [24,] 36163098 36163098
# [25,] 40327472 40327472
# [26,] 40183870 40183870
# [27,] 23370876 23370876
# [28,] 21104987 21104987
# [29,] 28901718 28901718
# [30,] 22919104 22919104
# [31,] 22972986 22972986
# [32,] 18753434 18753434
# [33,] 19830732 19830732
# [34,] 11894799 11894799
# [35,] 19682182 19682182
# [36,] 20931070 20931070
# [37,] 22455651 22455651
# [38,] 27749510 27749510
# [39,] ?        ?       Again weird formatting thing
